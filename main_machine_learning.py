#!/usr/bin/env python3
import numpy as np
from tqdm import tqdm
from sklearn.decomposition import IncrementalPCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from config import config
from src.dataloader import MRIDataset
from src.transform import transform_volume


def batch_indices(arr, batch_size):
    """ì¸ë±ìŠ¤ ë°°ì—´ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶„í• """
    for i in range(0, len(arr), batch_size):
        yield arr[i : i + batch_size]


def load_batch(dataset, idxs):
    """
    ì£¼ì–´ì§„ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë°ì´í„° ë°°ì¹˜ë¥¼ ë¡œë“œí•˜ê³  flattení•œ ndarray ë°˜í™˜
    """
    batch_size = len(idxs)
    # ì²« ë³¼ë¥¨ í¬ê¸°ë¡œ ì´ˆê¸°í™”
    vol0, _ = dataset[idxs[0]]
    vol_np0 = vol0.numpy() if hasattr(vol0, "numpy") else vol0
    flat_size = int(np.prod(vol_np0.shape))
    X = np.zeros((batch_size, flat_size), dtype=np.float32)
    y = np.zeros(batch_size, dtype=int)
    for j, idx in enumerate(idxs):
        vol, label = dataset[idx]
        vol_np = vol.numpy() if hasattr(vol, "numpy") else vol
        X[j] = vol_np.astype(np.float32).flatten()
        y[j] = int(label)
    return X, y


def main():
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset_dir = config.root_dir_A if config.mode == "A" else config.root_dir_B
    dataset = MRIDataset(dataset_dir, config, transform=transform_volume)
    N = len(dataset)

    # ì „ì²´ ë ˆì´ë¸” ëª©ë¡ ì¶”ì¶œ
    labels = np.array([dataset.samples[i][1] for i in range(N)])

    # train/test split
    indices = np.arange(N)
    train_idx, test_idx = train_test_split(indices, stratify=labels, test_size=0.2, random_state=42)
    n_train, n_test = len(train_idx), len(test_idx)

    # IncrementalPCA í•™ìŠµ
    n_components = 50
    print("ğŸ” IncrementalPCA í•™ìŠµ ì¤‘...")
    ipca = IncrementalPCA(n_components=n_components)
    for batch_idx in tqdm(batch_indices(train_idx, batch_size=100)):
        X_batch, _ = load_batch(dataset, batch_idx)
        ipca.partial_fit(X_batch)

    # PCA ë³€í™˜ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ ë‚´ ndarrayë¡œ ìƒì„±
    X_train = np.zeros((n_train, n_components), dtype=np.float32)
    X_test = np.zeros((n_test, n_components), dtype=np.float32)
    y_train = labels[train_idx]
    y_test = labels[test_idx]

    print("ğŸ”„ PCA ë³€í™˜ ì¤‘...")
    # train ë³€í™˜
    for i, batch_idx in enumerate(tqdm(batch_indices(train_idx, batch_size=100))):
        X_batch, _ = load_batch(dataset, batch_idx)
        X_train[i * 100 : (i * 100 + len(batch_idx))] = ipca.transform(X_batch)
    # test ë³€í™˜
    for i, batch_idx in enumerate(tqdm(batch_indices(test_idx, batch_size=100))):
        X_batch, _ = load_batch(dataset, batch_idx)
        X_test[i * 100 : (i * 100 + len(batch_idx))] = ipca.transform(X_batch)

    # ëª¨ë¸ ì •ì˜
    models = {
        "RandomForest": RandomForestClassifier(n_estimators=100, random_state=42),
        "SVM (RBF Kernel)": SVC(kernel="rbf", probability=True, random_state=42),
        "LogisticRegression": LogisticRegression(max_iter=1000, random_state=42),
        "SGDClassifier": SGDClassifier(loss="log_loss", max_iter=1000, random_state=42),
    }

    # í•™ìŠµ ë° í‰ê°€
    for name, model in models.items():
        print(f"ğŸ§  {name} í•™ìŠµ ë° í‰ê°€ ì¤‘...")
        if hasattr(model, "partial_fit") and name == "SGDClassifier":
            model.partial_fit(X_train, y_train, classes=np.unique(y_train))
        else:
            model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        report = classification_report(y_test, y_pred, target_names=["ì¹˜ë§¤(0)", "ì •ìƒ(1)", "ê²½ì¦ì¹˜ë§¤(2)"])
        print(f"\nğŸ“Œ {name} ì„±ëŠ¥:\n{report}")


if __name__ == "__main__":
    main()
